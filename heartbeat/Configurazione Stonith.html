<p>Stonith è un componente essenziale di Heartbeat/Pacemaker, e serve ad accertarsi che un nodo non possa prendere in carico le risorse, questo per prevenire un accesso contemporaneo ai dai che in caso di accesso in scrittura significa quasi certamente la perdita delle informazioni.</p>
<p>Ci sono diversi plugin per stonith. Il più semplice è ssh, ma può essere usato unicamente in ambiente di test (e nemmeno del tutto...). Questo plugin non fa altro che lanciare un reboot sull&#39;altra macchina, ed è evidente che risulta inaffidabile perché se l&#39;altro nodo non risponde oppure è disconnesso dalla rete il comando di stonith non potrà andare a buon fine. Se il comando non torna OK il cluster non considera il nodo come spento e rimane <em>paralizzato</em>. </p>
<p>Ad esempio se configuriamo uno stonith ssh, togliamo la corrente al nodo attivo, il nodo di failover lancerà stonith che fallirà. Non essendo sicuro che l&#39;altro nodo sia effettivamente fermo o riavviato, il nodo in standby non potrà prendere in carico i servizi e rimarrà fermo.</p>
<p> </p>
<p>Il fencing più affidabile è quello che taglia la corrente, ad esempio tramite UPS (nut, apc) oppure meglio tramite delle <a href="http://en.wikipedia.org/wiki/Power_distribution_unit">PDU</a> comandate via rete.</p>
<p>Altri fencing affidabili, in caso di macchine virtuali, possono esser l&#39;uso dell&#39;host fisico dove sono eseguite le VM.</p>
<p> </p>
<h3>Info sulla configurazione</h3>
<p>I comandi stonith sono delle primitive, proprio come le risorse da gestire. L&#39;unica differenza è che, ad esempio, la risorsa che serve a spegnere <em>host1</em> non dovrà mai girare su <em>host1</em> stesso. Per fare questo si definisce la primitiva con nome, ad esempio, <em>st_host1</em> che comanda lo spegnimento di <em>host1</em>, poi si definisce una limitazione di location:</p>
<pre class="brush: bash;fontsize: 100; first-line: 1; ">primitive st_host1 stonith:external/libvirt \
  params hostlist="host1" \
   hypervisor_uri="qemu+tcp://HOSTSERVER/system" \
  op monitor interval="60"
location l_st_host1 st_host1 -inf: host1</pre>
<p> </p>
<p> </p>
<h3>Fencing con libvirtd</h3>
<p>L&#39;accesso a libvritd è fattibile tramite ssh o tramite tcp. Per evitare di dover copiare le chiavi ssh dalle vm agli host meglio usare tcp.</p>
<p>Modificare il file <em>/etc/default/libvirt-bin</em> aggiungendo <em>-l</em> al parametro <em>libvirtd_opts</em>.</p>
<p>Modificare /etc/libvirt/libvirtd.conf impostando questi parametri:</p>
<pre class="brush: bash;fontsize: 100; first-line: 1; ">listen_tls = 0
listen_tcp = 1
tcp_port = "16509"
auth_tcp = "none"</pre>
<p>riavviare libvirtd con il comando</p>
<pre class="brush: bash;fontsize: 100; first-line: 1; ">restart libvirtd</pre>
<p>se sono presenti due host effettuare queste operazioni anche sulla seconda macchina.</p>
<p>Per lanciare i comandi serve il client virsh, ottenibile sulle vm con il comando</p>
<pre class="brush: bash;fontsize: 100; first-line: 1; ">apt-get install libvirt-bin</pre>
<p>Per verificare la configurazione eseguire:</p>
<pre class="brush: bash;fontsize: 100; first-line: 1; ">virsh --connect=qemu+tcp://HOSTFISICO/system list --all</pre>
<p>dove HOSTFISICO è il server dove gira libvirtd.</p>
<p>Quindi lanciare la configurazione di heartbeat e configurare le primitive stonith:</p>
<pre class="brush: bash;fontsize: 100; first-line: 1; ">primitive st_kvm_u1 stonith:external/libvirt \
        params hostlist="ubuntu1:u1" hypervisor_uri="qemu+tcp://10.22.22.159/system"
primitive st_kvm_u2 stonith:external/libvirt \
        params hostlist="ubuntu2:u2" hypervisor_uri="qemu+tcp://10.22.22.158/system"
</pre>
<p>nel campo <em>hostlist</em> va messo il nome della risorsa del cluster. La parte dopo i due punti è opzionale, e serve a fornire l&#39;id della vm in caso non corrisponda al nome della risorsa. In questo esempio la risorsa heartbeat si chiama <em>ubuntu1</em> mentre la vm è <em>u1</em>.</p>
<p>In caso si preferisca usare libvirt tramite ssh sostituire <em>qemu+tcp</em> con <em>qemu+ssh</em>.</p>
<p> </p>
<p>rif: <a href="http://www.hastexo.com/resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes">http://www.hastexo.com/resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes</a></p>
<p> </p>
<h3>Fencing con Nut</h3>
<p><a href="http://www.nevis.columbia.edu/twiki/bin/view/Nevis/CorosyncSinglePrimaryConfiguration">http://www.nevis.columbia.edu/twiki/bin/view/Nevis/CorosyncSinglePrimaryConfiguration</a></p>
<h3> </h3>
<h3>Fencing con IPMI</h3>
<p>IMPI è il dispositivo di controllo hardware dei server.</p>
<p>Prima di tutto <a href="https://mail.ufficyo.com/kb/index.php?action=artikel&amp;cat=4&amp;id=439&amp;artlang=it&amp;">confiugurarlo</a>: </p>
<p>Quindi:</p>
<p><a href="http://www.karlkatzke.com/ipmi-stonith-howto-with-pacemaker/">http://www.karlkatzke.com/ipmi-stonith-howto-with-pacemaker/</a></p>
<p> </p>
<p> </p>
<p> </p>
<p>rif: <a href="http://doc.opensuse.org/products/draft/SLE-HA/SLE-ha-guide_sd_draft/cha.ha.fencing.html">http://doc.opensuse.org/products/draft/SLE-HA/SLE-ha-guide_sd_draft/cha.ha.fencing.html</a></p>